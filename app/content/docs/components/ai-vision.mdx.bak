---
title: AI Vision
description: A comprehensive AI-powered image analysis component with object detection, OCR, scene description, and visual overlays.
featured: true
component: true
---

<ComponentPreview name="ai-vision-demo" description="AI-powered image analysis with visual overlays" />

## Installation

<Tabs defaultValue="cli">

<TabsList>
  <TabsTrigger value="cli">CLI</TabsTrigger>
  <TabsTrigger value="manual">Manual</TabsTrigger>
</TabsList>
<TabsContent value="cli">

```bash
npx hanzo-ui@latest add ai-vision
```

</TabsContent>

<TabsContent value="manual">

<Steps>

<Step>Install the following dependencies:</Step>

```bash
npm install @radix-ui/react-tabs @radix-ui/react-slider @radix-ui/react-switch @radix-ui/react-dropdown-menu @radix-ui/react-scroll-area lucide-react class-variance-authority
```

<Step>Copy and paste the following code into your project.</Step>

<ComponentSource name="ai-vision" />

<Step>Update the import paths to match your project setup.</Step>

</Steps>

</TabsContent>

</Tabs>

## Usage

```tsx
import { AIVision } from "@/components/ui/ai-vision"

export function AIVisionDemo() {
  const handleAnalysis = (results) => {
    console.log("Analysis results:", results)
  }

  return (
    <AIVision
      onAnalysis={handleAnalysis}
      capabilities={[
        "object-detection",
        "scene-description",
        "ocr",
        "color-analysis",
        "tagging",
        "face-detection",
        "sentiment-analysis"
      ]}
      enableCamera={true}
      enableUrl={true}
      showConfidence={true}
      minConfidence={0.5}
    />
  )
}
```

## Features

### Input Methods
- **File Upload**: Drag and drop or click to upload images
- **Camera Capture**: Take photos directly from the camera
- **URL Input**: Load images from web URLs
- **Image Generation**: Generate images from text prompts (optional)

### Analysis Capabilities
- **Object Detection**: Identify and locate objects with bounding boxes
- **Scene Description**: Generate natural language descriptions of images
- **OCR (Text Recognition)**: Extract text from images with positioning
- **Color Analysis**: Extract dominant color palettes
- **Image Tagging**: Generate relevant tags and labels
- **Face Detection**: Detect faces with emotion and demographic analysis
- **Sentiment Analysis**: Analyze emotional content of images

### Visualization Features
- **Canvas Overlays**: Visual bounding boxes and annotations
- **Confidence Scores**: Display confidence levels for detections
- **Interactive Controls**: Toggle overlay types and adjust thresholds
- **Color Coding**: Different colors for different detection types

## Examples

### Basic Usage

```tsx
import { AIVision } from "@/components/ui/ai-vision"

export function BasicVision() {
  return (
    <AIVision
      onAnalysis={(results) => {
        console.log("Objects detected:", results.objects?.length)
        console.log("Scene:", results.description)
      }}
    />
  )
}
```

### Custom Configuration

```tsx
import { AIVision } from "@/components/ui/ai-vision"

export function CustomVision() {
  return (
    <AIVision
      capabilities={["object-detection", "ocr", "face-detection"]}
      maxFileSize={5 * 1024 * 1024} // 5MB
      acceptedFormats={["image/jpeg", "image/png"]}
      apiEndpoint="/api/custom-vision"
      enableCamera={false}
      enableGeneration={true}
      minConfidence={0.7}
      showConfidence={true}
    />
  )
}
```

### With Analysis Handler

```tsx
import { AIVision, VisionResults } from "@/components/ui/ai-vision"
import { useState } from "react"

export function VisionWithHandler() {
  const [results, setResults] = useState<VisionResults | null>(null)

  const handleAnalysis = (analysisResults: VisionResults) => {
    setResults(analysisResults)

    // Process specific results
    if (analysisResults.objects) {
      console.log(`Found ${analysisResults.objects.length} objects`)
    }

    if (analysisResults.text) {
      const extractedText = analysisResults.text.map(t => t.text).join(' ')
      console.log("Extracted text:", extractedText)
    }

    if (analysisResults.faces) {
      console.log(`Detected ${analysisResults.faces.length} faces`)
    }
  }

  return (
    <div className="space-y-4">
      <AIVision onAnalysis={handleAnalysis} />

      {results && (
        <div className="p-4 border rounded-lg">
          <h3 className="font-semibold mb-2">Quick Summary</h3>
          <p>Objects: {results.objects?.length || 0}</p>
          <p>Text regions: {results.text?.length || 0}</p>
          <p>Faces: {results.faces?.length || 0}</p>
          {results.description && (
            <p className="mt-2 text-sm text-muted-foreground">
              {results.description}
            </p>
          )}
        </div>
      )}
    </div>
  )
}
```

## API Reference

### Props

<PropsTable>
  <PropDef name="onAnalysis" type="(results: VisionResults) => void" required={false}>
    Callback function called when image analysis completes
  </PropDef>
  <PropDef name="capabilities" type="VisionCapability[]" required={false} defaultValue='["object-detection", "scene-description", "ocr", "color-analysis", "tagging"]'>
    Array of vision capabilities to enable
  </PropDef>
  <PropDef name="maxFileSize" type="number" required={false} defaultValue="10485760">
    Maximum file size in bytes (default: 10MB)
  </PropDef>
  <PropDef name="acceptedFormats" type="string[]" required={false} defaultValue='["image/jpeg", "image/png", "image/webp", "image/gif"]'>
    Accepted image file formats
  </PropDef>
  <PropDef name="apiEndpoint" type="string" required={false} defaultValue="/api/vision">
    API endpoint for image analysis
  </PropDef>
  <PropDef name="apiKey" type="string" required={false}>
    API key for vision service authentication
  </PropDef>
  <PropDef name="defaultImage" type="string" required={false}>
    Default image URL to load on component mount
  </PropDef>
  <PropDef name="enableCamera" type="boolean" required={false} defaultValue="true">
    Enable camera capture functionality
  </PropDef>
  <PropDef name="enableUrl" type="boolean" required={false} defaultValue="true">
    Enable URL input for remote images
  </PropDef>
  <PropDef name="enableGeneration" type="boolean" required={false} defaultValue="false">
    Enable AI image generation from text
  </PropDef>
  <PropDef name="enableEditing" type="boolean" required={false} defaultValue="false">
    Enable AI-powered image editing
  </PropDef>
  <PropDef name="showConfidence" type="boolean" required={false} defaultValue="true">
    Show confidence scores in overlays
  </PropDef>
  <PropDef name="minConfidence" type="number" required={false} defaultValue="0.5">
    Minimum confidence threshold for displaying results
  </PropDef>
</PropsTable>

### Types

#### VisionCapability

```tsx
type VisionCapability =
  | "object-detection"
  | "scene-description"
  | "ocr"
  | "color-analysis"
  | "tagging"
  | "face-detection"
  | "sentiment-analysis"
  | "image-generation"
  | "image-editing"
```

#### VisionResults

```tsx
interface VisionResults {
  objects?: BoundingBox[]
  description?: string
  text?: DetectedText[]
  colors?: ColorPalette[]
  tags?: { label: string; confidence: number }[]
  faces?: FaceDetection[]
  sentiment?: {
    emotion: string
    confidence: number
    valence: number
    arousal: number
  }
  metadata?: {
    width: number
    height: number
    format: string
    size: number
    exif?: Record<string, any>
  }
}
```

#### BoundingBox

```tsx
interface BoundingBox {
  x: number
  y: number
  width: number
  height: number
  confidence: number
  label: string
  color?: string
}
```

#### DetectedText

```tsx
interface DetectedText {
  text: string
  confidence: number
  boundingBox: BoundingBox
}
```

#### ColorPalette

```tsx
interface ColorPalette {
  color: string
  percentage: number
  name?: string
}
```

#### FaceDetection

```tsx
interface FaceDetection {
  boundingBox: BoundingBox
  confidence: number
  age?: number
  gender?: string
  emotion?: string
  landmarks?: { x: number; y: number }[]
}
```

## Integration

### Backend API

The component expects a vision API endpoint that accepts image data and returns analysis results:

```typescript
// Example API route (Next.js)
export async function POST(request: Request) {
  const formData = await request.formData()
  const image = formData.get('image') as File

  // Process image with your vision service
  const results = await analyzeImage(image)

  return Response.json(results)
}
```

### Custom Vision Services

You can integrate with various vision APIs:

- **OpenAI Vision API**
- **Google Cloud Vision**
- **Azure Computer Vision**
- **AWS Rekognition**
- **Custom ML models**

Example integration with OpenAI:

```typescript
import OpenAI from 'openai'

async function analyzeWithOpenAI(imageUrl: string) {
  const openai = new OpenAI()

  const response = await openai.chat.completions.create({
    model: "gpt-4-vision-preview",
    messages: [
      {
        role: "user",
        content: [
          { type: "text", text: "Analyze this image and provide detailed information about objects, text, and scene description." },
          { type: "image_url", image_url: { url: imageUrl } }
        ]
      }
    ]
  })

  return parseVisionResponse(response.choices[0].message.content)
}
```

## Accessibility

The component includes several accessibility features:

- **Keyboard Navigation**: Full keyboard support for all interactive elements
- **Screen Reader Support**: Proper ARIA labels and descriptions
- **Focus Management**: Clear focus indicators and logical tab order
- **High Contrast**: Works well with high contrast themes
- **Alternative Text**: Provides alternative descriptions for detected content

## Performance

### Optimization Tips

1. **Image Preprocessing**: Resize large images before analysis
2. **Selective Capabilities**: Only enable needed analysis capabilities
3. **Confidence Thresholds**: Use appropriate confidence levels to filter results
4. **Caching**: Cache results for previously analyzed images
5. **Lazy Loading**: Load analysis results progressively

### Example Optimization

```tsx
import { AIVision } from "@/components/ui/ai-vision"

export function OptimizedVision() {
  return (
    <AIVision
      capabilities={["object-detection", "ocr"]} // Only what you need
      minConfidence={0.7} // Higher threshold for fewer false positives
      maxFileSize={2 * 1024 * 1024} // Limit file size
      onAnalysis={(results) => {
        // Cache results
        localStorage.setItem(
          `vision-${Date.now()}`,
          JSON.stringify(results)
        )
      }}
    />
  )
}
```

## Troubleshooting

### Common Issues

1. **Camera Access Denied**: Ensure HTTPS and proper permissions
2. **Large File Uploads**: Check `maxFileSize` and server limits
3. **API Timeout**: Implement proper error handling and retries
4. **Canvas Overlay Positioning**: Ensure image dimensions are properly calculated

### Error Handling

```tsx
import { AIVision } from "@/components/ui/ai-vision"
import { toast } from "@/components/ui/use-toast"

export function VisionWithErrorHandling() {
  const handleAnalysis = (results: VisionResults) => {
    try {
      // Process results
      console.log(results)
    } catch (error) {
      toast({
        title: "Analysis Error",
        description: "Failed to process vision results",
        variant: "destructive"
      })
    }
  }

  return (
    <AIVision
      onAnalysis={handleAnalysis}
      apiEndpoint="/api/vision"
    />
  )
}
```